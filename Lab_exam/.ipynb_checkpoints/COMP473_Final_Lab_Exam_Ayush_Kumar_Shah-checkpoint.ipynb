{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques no 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'would', 'said', 'new', 'could', 'time', 'two', 'may', 'first', 'like', 'man', 'even', 'made', 'also', 'many', 'must', 'af', 'back', 'years', 'much', 'way', 'well', 'people', 'mr.', 'little', 'state', 'good', 'make', 'world', 'still', 'see', 'men', 'work', 'long', 'get', 'life', 'never', 'day', 'another', 'know', 'last', 'us', 'might', 'great', 'old', 'year', 'come', 'since', 'go', 'came']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def most_frequent_content_words(text):\n",
    "    content_words = [w.lower() for w in text if w.lower() not in stopwords and any(c.isalpha() for c in w)]\n",
    "    fd = nltk.FreqDist(content_words)\n",
    "    return [w for w, num in fd.most_common(50)]\n",
    "text=brown.words()\n",
    "print (most_frequent_content_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques no 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colourless'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s = s[:4] + 'u' + s[4:]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques no 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent=\"She sells sea shells by the sea shore\"\n",
    "words=nltk.word_tokenize(sent)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shells', 'shore']\n",
      "['sells', 'shells', 'shore']\n"
     ]
    }
   ],
   "source": [
    "print([w for w in words if w.startswith('sh')])\n",
    "print([w for w in words if len(w) > 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques no 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('wind', 'VBP'),\n",
       " ('back', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('clock', 'NN'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('chase', 'VBP'),\n",
       " ('after', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wind', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'They wind back the clock, while we chase after the wind.'\n",
    "nltk.pos_tag(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronounciations involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they [['DH', 'EY1']]\n",
      "wind [['W', 'AY1', 'N', 'D'], ['W', 'IH1', 'N', 'D']]\n",
      "back [['B', 'AE1', 'K']]\n",
      "the [['DH', 'AH0'], ['DH', 'AH1'], ['DH', 'IY0']]\n",
      "clock [['K', 'L', 'AA1', 'K']]\n",
      "while [['W', 'AY1', 'L'], ['HH', 'W', 'AY1', 'L']]\n",
      "we [['W', 'IY1']]\n",
      "chase [['CH', 'EY1', 'S']]\n",
      "after [['AE1', 'F', 'T', 'ER0']]\n",
      "the [['DH', 'AH0'], ['DH', 'AH1'], ['DH', 'IY0']]\n",
      "wind [['W', 'AY1', 'N', 'D'], ['W', 'IH1', 'N', 'D']]\n"
     ]
    }
   ],
   "source": [
    "words=nltk.word_tokenize(sent)\n",
    "alphabetical_words=[word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "pronounciations = nltk.corpus.cmudict.dict()\n",
    "for word in alphabetical_words:\n",
    "   print(word,pronounciations[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques no 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> porter = nltk.PorterStemmer()\n",
    ">>> [porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ques 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4160"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('While', 'CS'),\n",
       " ('availability', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('mortgage', 'NN'),\n",
       " ('money', 'NN'),\n",
       " ('has', 'HVZ'),\n",
       " ('been', 'BEN'),\n",
       " ('a', 'AT'),\n",
       " ('factor', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('encouraging', 'VBG'),\n",
       " ('apartment', 'NN'),\n",
       " ('construction', 'NN'),\n",
       " (',', ','),\n",
       " ('the', 'AT'),\n",
       " ('generally', 'RB'),\n",
       " ('high', 'JJ'),\n",
       " ('level', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('prosperity', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('past', 'NN'),\n",
       " ('few', None),\n",
       " ('years', None),\n",
       " ('plus', None),\n",
       " ('rising', None),\n",
       " ('consumer', None),\n",
       " ('income', None),\n",
       " ('are', None),\n",
       " ('among', None),\n",
       " ('the', None),\n",
       " ('factors', None),\n",
       " ('that', None),\n",
       " ('have', None),\n",
       " ('encouraged', None),\n",
       " ('builders', None),\n",
       " ('to', None),\n",
       " ('concentrate', None),\n",
       " ('in', None),\n",
       " ('the', None),\n",
       " ('apartment-building', None),\n",
       " ('field', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "bigram_tagger.tag(brown_sents[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 'CC'),\n",
       " ('it', 'PPS'),\n",
       " ('was', 'BEDZ'),\n",
       " ('filled', None),\n",
       " ('then', None),\n",
       " ('as', None),\n",
       " ('now', None),\n",
       " ('by', None),\n",
       " ('quarreling', None),\n",
       " ('tribes', None),\n",
       " ('with', None),\n",
       " ('no', None),\n",
       " ('political', None),\n",
       " ('or', None),\n",
       " ('historical', None),\n",
       " ('unity', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_sent = brown_sents[4200]\n",
    "bigram_tagger.tag(unseen_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10206319146815508"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884137382485832"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tagging accuracy is higher with training data than the test data. The bigram tagger manages to tag every word in a sentence it saw during training, but does badly on an unseen sentence since when it encounters a new word , it is unable to assign a tag to the unseen word. So, the tag 'None' is assigned as seen in the above example in unseen_sent.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a result, it cannot tag the following word even if it was seen during training, because it never saw it during training with the tag i.e. 'None' on the previous word. Consequently, the tagger fails to tag the rest of the sentence as seen in the example above. Hence, its overall accuracy score is very low for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
